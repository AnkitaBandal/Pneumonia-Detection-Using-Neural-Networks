# Markdown Cell
# Library Installation and Extracting Files from Google Drive

# Code Cell
!pip install --upgrade tensorflow
!pip install --upgrade keras
!pip install --upgrade torch torchvision torchaudio

# Code Cell
# Importing required Libraries

import numpy as np
import pandas as pd
import os
import tensorflow as tf
import keras
import warnings
import matplotlib.pyplot as plt
import cv2
from tqdm.keras import TqdmCallback
from tensorflow.keras import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras import models
from keras import layers
from keras.applications import resnet50
from keras.models import *
from keras.layers import *
from tensorflow.keras.applications.vgg19 import VGG19
from keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications import InceptionV3
warnings.filterwarnings('ignore')

# Code Cell
# Mounting Google Drive

from google.colab import drive
drive.mount('/content/drive')

# Code Cell
# Extracting Data from zip file

import zipfile
from tqdm.notebook import tqdm

zip_path = '/content/drive/My Drive/chest_xray.zip'
extract_to = '/content/xray_images'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    files_list = zip_ref.infolist()
    with tqdm(total=len(files_list), desc="Extracting files") as pbar:
        for file in files_list:
            zip_ref.extract(member=file, path=extract_to)
            pbar.update(1)

# Code Cell
# List the contents of the xray_images directory

!ls "/content/xray_images/chest_xray"

# Code Cell
# Setting Path File and Directory

def data_prep(path_pneumonia, path_normal):
    files_normal = [f for f in os.listdir(path_normal) if not f.startswith('.')]
    files_pneumonia = [f for f in os.listdir(path_pneumonia) if not f.startswith('.')]

    pneumonia = [os.path.join(path_pneumonia, f) for f in files_pneumonia]
    normal = [os.path.join(path_normal, f) for f in files_normal]

    labels = ['pneumonia'] * len(pneumonia) + ['normal'] * len(normal)
    data = pneumonia + normal

    return pd.DataFrame({'Image_Path': data, 'Labels': labels})

path_pneumonia = '/content/xray_images/chest_xray/PNEUMONIA'
path_normal = '/content/xray_images/chest_xray/NORMAL'

df = data_prep(path_pneumonia, path_normal)
print(df.head())

# Code Cell
# Displaying Normal and Pneumonia Chest X-Rays

print("Displaying 5 Pneumonia Chest X-Rays: ")
plt.figure(figsize=(20, 4))
for i in range(5):
    img_path = df[df['Labels'] == 'pneumonia']['Image_Path'].iloc[i]
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(1, 5, i + 1)
    plt.imshow(img)
    plt.axis('off')
plt.tight_layout()
plt.show()

print("\nDisplaying 5 Normal Chest X-Rays: ")
plt.figure(figsize=(20, 4))
for i in range(5):
    img_path = df[df['Labels'] == 'normal']['Image_Path'].iloc[i]
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(1, 5, i + 1)
    plt.imshow(img)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Code Cell
# Counting Labels of each Category

label_counts = df['Labels'].value_counts()
print(label_counts)

# Code Cell
# Undersampling Pneumonia to match the Normal Data

df_pneumonia = df[df['Labels'] == 'pneumonia']
df_normal = df[df['Labels'] == 'normal']

df_pneumonia_undersampled = df_pneumonia.sample(n=1583, random_state=42)

df_balanced = pd.concat([df_pneumonia_undersampled, df_normal])

df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

df = df_balanced

print(df['Labels'].value_counts())

# Markdown Cell
# Data Preparation and Model Callback Configuration

# Code Cell
# Splitting Data into Train and Test Set

train, test = train_test_split(df, test_size=0.20)

# Code Cell
# Defines an ImageDataGenerator for data augmentation

img_datagen = ImageDataGenerator(
    rotation_range = 20,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    shear_range = 0.1,
    zoom_range = 0.1,
    horizontal_flip = True,
    fill_mode = 'nearest',
    validation_split = 0.2
)

# Code Cell
# Callback Function

my_callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=2),
    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.keras'),
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
]

# Code Cell
# Creating training and validation data generators

train_generator = img_datagen.flow_from_dataframe(
    dataframe = train,
    directory = None,
    x_col = 'Image_Path',
    y_col = 'Labels',
    target_size = (128, 128),
    color_mode = 'rgb',
    class_mode = 'binary',
    batch_size = 32,
    subset = 'training'
)

validation_generator = img_datagen.flow_from_dataframe(
    dataframe = train,
    directory = None,
    x_col = 'Image_Path',
    y_col = 'Labels',
    target_size = (128, 128),
    color_mode = 'rgb',
    class_mode = 'binary',
    batch_size = 32,
    subset = 'validation'
)

# Markdown Cell
# Model Performance Metrics Visualization

# Code Cell
# Visualization of all Performance Metrics

def plot_model_performance_metrics(model_history):
    accuracy = model_history.history['accuracy']
    val_accuracy = model_history.history['val_accuracy']
    loss = model_history.history['loss']
    val_loss = model_history.history['val_loss']
    precision_key = next((k for k in model_history.history.keys() if k.startswith('precision')), None)
    val_precision_key = next((k for k in model_history.history.keys() if k.startswith('val_precision')), None)
    recall_key = next((k for k in model_history.history.keys() if k.startswith('recall')), None)
    val_recall_key = next((k for k in model_history.history.keys() if k.startswith('val_recall')), None)

    plt.figure(figsize = (20, 10))

    plt.subplot(2, 3, 1)
    plt.plot(accuracy, label = 'Training Accuracy')
    plt.plot(val_accuracy, label = 'Validation Accuracy')
    plt.title('Accuracy: Training vs. Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(2, 3, 2)
    plt.plot(loss, label = 'Training Loss')
    plt.plot(val_loss, label = 'Validation Loss')
    plt.title('Loss: Training vs. Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    if precision_key and val_precision_key:
        precision = model_history.history[precision_key]
        val_precision = model_history.history[val_precision_key]
        plt.subplot(2, 3, 3)
        plt.plot(precision, label = 'Training Precision')
        plt.plot(val_precision, label = 'Validation Precision')
        plt.title('Precision: Training vs. Validation')
        plt.xlabel('Epochs')
        plt.ylabel('Precision')
        plt.legend()

    if recall_key and val_recall_key:
        recall = model_history.history[recall_key]
        val_recall = model_history.history[val_recall_key]
        plt.subplot(2, 3, 4)
        plt.plot(recall, label = 'Training Recall')
        plt.plot(val_recall, label = 'Validation Recall')
        plt.title('Recall: Training vs. Validation')
        plt.xlabel('Epochs')
        plt.ylabel('Recall')
        plt.legend()

    plt.tight_layout()
    plt.show()

# Markdown Cell
# Models

# Markdown Cell
## CNN Model

# Code Cell
def cnn_model(df, train, test, my_callbacks):
    x_train = img_datagen.flow_from_dataframe(
        dataframe = train,
        x_col = 'Image_Path',
        y_col = 'Labels',
        class_mode = 'binary',
        target_size = (134, 134),
        shuffle = True,
        batch_size = 10,
        seed = 10,
        subset = 'training')

    x_test = img_datagen.flow_from_dataframe(
        dataframe = test,
        x_col = 'Image_Path',
        y_col = 'Labels',
        class_mode = 'binary',
        target_size = (134, 134),
        shuffle = False,
        batch_size = 10,
        seed = 10)

    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu',
                               padding = 'same',
                               input_shape = (134, 134, 3)),
        tf.keras.layers.MaxPooling2D(2, 2),

        tf.keras.layers.Conv2D(64, (3, 3),
                               activation = 'relu',
                               padding = 'same'),
        tf.keras.layers.MaxPooling2D(2, 2),

        tf.keras.layers.Conv2D(128, (3, 3),
                               activation = 'relu',
                               padding = 'same'),
        tf.keras.layers.MaxPooling2D(2, 2),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation = 'relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(1, activation = 'sigmoid')
    ])

    model.compile(optimizer = tf.keras.optimizers.Adam(),
                  loss = 'binary_crossentropy',
                  metrics = ['accuracy',
                             tf.keras.metrics.Precision(),
                             tf.keras.metrics.Recall()])

    cnn_model_history = model.fit(
        x_train,
        steps_per_epoch = len(x_train),
        epochs = 5,
        validation_data = x_test,
        validation_steps = len(x_test),
        callbacks = my_callbacks)

    return cnn_model_history

# Code Cell
cnn_model_history = cnn_model(df, train, test, my_callbacks)
plot_model_performance_metrics(cnn_model_history)

# Markdown Cell
## ResNet Model

# Code Cell
def resnet_model(df, train, test, my_callbacks):
    x_train = img_datagen.flow_from_dataframe(
        dataframe = train,
        x_col = 'Image_Path',
        y_col = 'Labels',
        class_mode = 'binary',
        target_size = (224, 224),
        shuffle = True,
        batch_size = 10,
        seed = 10,
        subset = 'training')

    x_test = img_datagen.flow_from_dataframe(
        dataframe = test,
        x_col = 'Image_Path',
        y_col = 'Labels',
        class_mode = 'binary',
        target_size = (224, 224),
        shuffle = False,
        batch_size = 10,
        seed = 10)

    resnet_base = ResNet50(weights = 'imagenet',
                           include_top = False,
                           input_shape = (224, 224, 3))

    for layer in resnet_base.layers:
        layer.trainable = False

    model = Sequential([
        resnet_base,
        Flatten(),
        Dense(1024, activation = 'relu'),
        Dropout(0.5),
        Dense(1, activation = 'sigmoid')
    ])

    model.compile(optimizer = RMSprop(learning_rate=0.0001),
                  loss = 'binary_crossentropy',
                  metrics = ['accuracy',
                             tf.keras.metrics.Precision(),
                             tf.keras.metrics.Recall()])

    resnet_model_history = model.fit(
        x_train,
        steps_per_epoch = len(x_train),
        epochs = 5,
        validation_data = x_test,
        validation_steps = len(x_test),
        callbacks = my_callbacks)

    return resnet_model_history

# Code Cell
resnet_model_history = resnet_model(df, train, test, my_callbacks)
plot_model_performance_metrics(resnet_model_history)

# Markdown Cell
## VGG Model

# Code Cell
def vgg_model(df, train, test, my_callbacks):
    x_train = img_datagen.flow_from_dataframe(
        dataframe = train,
        x_col = 'Image_Path',
        y_col = 'Labels',
        target_size = (224, 224),
        class_mode = 'binary',
        batch_size = 10,
        shuffle = True,
        seed = 10,
        subset = 'training'
    )

    x_test = img_datagen.flow_from_dataframe(
        dataframe = test,
        x_col = 'Image_Path',
        y_col = 'Labels',
        target_size = (224, 224),
        class_mode = 'binary',
        batch_size = 10,
        shuffle = False,
        seed = 10,
        subset = 'validation'
    )

    vgg_base = VGG19(weights = 'imagenet',
                     include_top = False,
                     input_shape = (224, 224, 3))

    for layer in vgg_base.layers:
        layer.trainable = False

    model = Sequential([
        vgg_base,
        Flatten(),
        Dense(1024, activation = 'relu'),
        Dropout(0.5),
        Dense(1, activation = 'sigmoid')
    ])

    model.compile(optimizer = RMSprop(learning_rate=0.0001),
                  loss = 'binary_crossentropy',
                  metrics = ['accuracy',
                           tf.keras.metrics.Precision(),
                           tf.keras.metrics.Recall()])

    vgg_model_history = model.fit(
        x_train,
        steps_per_epoch = len(x_train),
        validation_data = x_test,
        validation_steps = len(x_test),
        epochs = 5,
        callbacks = my_callbacks
    )

    return vgg_model_history

# Code Cell
vgg_model_history = vgg_model(df, train, test, my_callbacks)
plot_model_performance_metrics(vgg_model_history)

# Markdown Cell
## Inception Model

# Code Cell
def inception_model(df, train, test, my_callbacks):
    x_train = img_datagen.flow_from_dataframe(
        dataframe = train,
        x_col = 'Image_Path',
        y_col = 'Labels',
        target_size = (299, 299),
        class_mode = 'binary',
        batch_size = 10,
        shuffle = True,
        seed = 10,
        subset = 'training'
    )

    x_test = img_datagen.flow_from_dataframe(
        dataframe=test,
        x_col = 'Image_Path',
        y_col = 'Labels',
        target_size = (299, 299),
        class_mode = 'binary',
        batch_size = 10,
        shuffle = False,
        seed = 10
    )

    inception_base = InceptionV3(weights = 'imagenet',
                                 include_top = False,
                                 input_shape = (299, 299, 3))

    for layer in inception_base.layers:
        layer.trainable = False

    model = Sequential([
        inception_base,
        Flatten(),
        Dense(1024, activation = 'relu'),
        Dropout(0.5),
        Dense(1, activation = 'sigmoid')
    ])

    model.compile(optimizer = RMSprop(learning_rate=0.0001),
                  loss = 'binary_crossentropy',
                  metrics = ['accuracy',
                           tf.keras.metrics.Precision(),
                           tf.keras.metrics.Recall()])

    inception_model_history = model.fit(
        x_train,
        steps_per_epoch = len(x_train),
        epochs = 5,
        validation_data = x_test,
        validation_steps = len(x_test),
        callbacks = my_callbacks
    )

    return inception_model_history

# Code Cell
inception_model_history = inception_model(df, train, test, my_callbacks)
plot_model_performance_metrics(inception_model_history)

# Markdown Cell
# Performance Evaluation and Model Selection

# Code Cell
pip install tabulate

# Code Cell
from tabulate import tabulate

model_histories = {
    'CNN Model': cnn_model_history,
    'VGG Model': vgg_model_history,
    'ResNet Model': resnet_model_history,
    'Inception Model': inception_model_history,
}

def get_final_metrics(history):
    metrics = history.history
    last_epoch = len(metrics['loss']) - 1
    results = {
        'Loss': metrics.get('loss')[last_epoch] if 'loss' in metrics else None,
        'Accuracy': metrics.get('accuracy')[last_epoch] if 'accuracy' in metrics else None,
        'Precision': metrics.get(next((k for k in metrics if 'precision' in k), None))[last_epoch] if any('precision' in k for k in metrics) else None,
        'Recall': metrics.get(next((k for k in metrics if 'recall' in k), None))[last_epoch] if any('recall' in k for k in metrics) else None,
        'Val Loss': metrics.get('val_loss')[last_epoch] if 'val_loss' in metrics else None,
        'Val Accuracy': metrics.get('val_accuracy')[last_epoch] if 'val_accuracy' in metrics else None,
        'Val Precision': metrics.get(next((k for k in metrics if 'val_precision' in k), None))[last_epoch] if any('val_precision' in k for k in metrics) else None,
        'Val Recall': metrics.get(next((k for k in metrics if 'val_recall' in k), None))[last_epoch] if any('val_recall' in k for k in metrics) else None,
    }
    return results

final_metrics = {model_name: get_final_metrics(history) for model_name, history in model_histories.items()}

data_for_tabulation = []
for model_name, metrics in final_metrics.items():
    row = [model_name]
    for key in ['Loss', 'Accuracy', 'Precision', 'Recall', 'Val Loss', 'Val Accuracy', 'Val Precision', 'Val Recall']:
        if key in ['Accuracy', 'Precision', 'Recall', 'Val Accuracy', 'Val Precision', 'Val Recall'] and metrics[key] is not None:
            row.append(f"{metrics[key] * 100:.2f}%")
        elif metrics[key] is not None:
            row.append(f"{metrics[key]:.3f}")
        else:
            row.append("N/A")
    data_for_tabulation.append(row)

headers = ["Model", "Loss", "Accuracy", "Precision", "Recall", "Val Loss",
           "Val Accuracy", "Val Precision", "Val Recall"]

table = tabulate(data_for_tabulation, headers=headers, tablefmt="fancy_grid")

print(table)

# Markdown Cell
## **Best Model:**

The **ResNet Model** has been selected as the best model due to its:

* ResNet has the highest validation accuracy (94.37%) among the models, indicating it generalizes better on unseen data compared to the other models.

* ResNet has the highest recall (94.26%), meaning it is best at identifying positive cases (e.g., pneumonia).
* ResNet has Balanced Precision provides a good trade-off between precision and recall.
* ResNet has a low validation loss (0.335), which suggests it has better optimization and is not overfitting compared to models with higher validation loss, like VGG (1.04)







